%% LyX 1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,12pt,a4paper,twoside]{book}
\usepackage{times}
%\usepackage{algorithm2e}
\usepackage{url}
\usepackage{bbm}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
%\geometry{verbose,letterpaper,tmargin=2.5cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{pdfpages}

%\usepackage{xcolor,framed}
%\colorlet{shadecolor}{blue!10}
%\begin{shaded}blabla\end{shaded}

%\usepackage{xr}
%\externaldocument{PRS-supp}

%\linenumbers
%\doublespacing
\onehalfspacing
\usepackage[authoryear]{natbib}
%\usepackage{natbib}
%\usepackage{chapterbib}

\usepackage{numprint}
\npthousandsep{,}

%Pour les rajouts
\usepackage{xcolor}

\usepackage{dsfont}
\usepackage[warn]{textcomp}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{{figures/}}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\let\tabbeg\tabular
\let\tabend\endtabular
\renewenvironment{tabular}{\begin{adjustbox}{max width=0.75\textwidth}\tabbeg}{\tabend\end{adjustbox}}

\makeatletter

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Bold symbol macro for standard LaTeX users
%\newcommand{\boldsymbol}[1]{\mbox{\boldmath $#1$}}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\definecolor{clumping}{HTML}{38761D}
\definecolor{thresholding}{HTML}{1515FF}
%<span style="color:#38761D">Clumping</span> + <span style="color:#1515FF">Thresholding</span>

\usepackage{babel}
\makeatother


\begin{document}

\includepdfset{offset=.15in 0in,noautoscale,scale=1,pages={-},pagecommand={}}
\includepdf{cover}

\afterpage{\blankpage}


\clearpage

\section*{Abstract}

Genotyping is becoming cheaper, making genotype data available for millions of individuals.
Moreover, imputation enables to get genotype information at millions of loci capturing most of the genetic variation in the human genome.
Given such large data and the fact that many traits and diseases are heritable (e.g.\ 80\% of the variation of height in the population can be explained by genetics), it is envisioned that predictive models based on genetic information will be part of a personalized medicine.

In my thesis work, I focused on improving predictive ability of polygenic models.
Because prediction modeling is part of a larger statistical analysis of datasets, I developed tools to allow flexible exploratory analyses of large datasets, which consist in two R/C++ packages described in the first part of my thesis.
Then, I developed some efficient implementation of penalized regression to build polygenic models based on hundreds of thousands of genotyped individuals.
Finally, I improved the ``clumping and thresholding'' method, which is the most widely used polygenic method and is based on summary statistics that are widely available as compared to individual-level data.

Overall, I applied many concepts of statistical learning to genetic data. I used extreme gradient boosting for imputing genotyped variants, feature engineering to capture recessive and dominant effects in penalized regression, and parameter tuning and stacked regressions to improve polygenic prediction.
Statistical learning is not widely used in human genetics and my thesis is an attempt to change that.


\clearpage

\section*{R\'esum\'e}

Le g\'enotypage devient de moins en moins cher, rendant les donn\'ees de g\'enotypes disponibles pour des millions d'individus.
Par ailleurs, l'imputation permet d'obtenir l'information  g\'enotypique pour des millions de positions de l'ADN, capturant l'essentiel de la variation g\'en\'etique du g\'enome humain.
Compte tenu de la richesse des donn\'ees et du fait que de nombreux traits et maladies sont h\'er\'editaires (par exemple, la g\'en\'etique peut expliquer 80\% de la variation de la taille dans la population), il est envisag\'e d'utiliser des mod\`eles pr\'edictifs bas\'es sur l'information g\'en\'etique dans le cadre d'une m\'edecine personnalis\'ee.

Au cours de ma th\`ese, je me suis concentr\'e sur l'am\'elioration de la capacit\'e pr\'edictive des mod\`eles polyg\'eniques.
Les mod\`eles pr\'edictifs faisant partie d'une analyse statistique plus large des jeux de donn\'ees, j'ai d\'evelopp\'e des outils permettant l'analyse exploratoire de grands jeux de donn\'ees, constitu\'es de deux packages R/C++ d\'ecrits dans la premi\`ere partie de ma th\`ese.
Ensuite, j'ai d\'evelopp\'e une impl\'ementation efficace de la r\'egression p\'enalis\'ee pour construire des mod\`eles polyg\'eniques bas\'es sur des centaines de milliers d'individus g\'enotyp\'es.
Enfin, j'ai am\'elior\'e la m\'ethode appel\'ee ``clumping and thresholding'', qui est la m\'ethode polyg\'enique la plus largement utilis\'ee et qui est bas\'ee sur des statistiques r\'esum\'ees plus largement accessibles par rapport aux donn\'ees individuelles.

Dans l'ensemble, j'ai appliqu\'e de nombreux concepts d'apprentissage statistique aux donn\'ees g\'en\'etiques. J'ai utilis\'e du ``extreme gradient boosting'' pour imputer des variants g\'enotyp\'es, du ``feature engineering'' pour capturer des effets r\'ecessifs et dominants dans une r\'egression p\'enalis\'ee, et du ``parameter tuning'' et des ``stacked regressions'' pour am\'eliorer les mod\`eles polyg\'eniques pr\'edictifs.
L'apprentissage statistique n'est pour l'instant pas tr\`es utilis\'e en g\'en\'etique humaine et ma th\`ese est une tentative pour changer cela.


\clearpage

\section*{Remerciements}

Je voudrais d'abord remercier le LabEx PERSYVAL-Lab, l'universit\'{e} Grenoble Alpes, l'\'{e}cole doctorale EDISCE et le laboratoire TIMC-IMAG pour m'avoir permis de faire cette th\`{e}se. J'aimerais ensuite remercier les membres du jury, Florence, Julien, Benoit et Laurent pour avoir accept\'{e} de faire partie du jury de th\`{e}se, s'\^{e}tre d\'{e}plac\'{e} pour ma soutenance et s'\^{e}tre int\'{e}ress\'{e} \`{a} mon travail de th\`{e}se. J'aimerais aussi remercier les membres de mon comit\'{e} de suivi de th\`{e}se, Thomas et Julien, pour avoir suivi mes travaux de th\`{e}se et assur\'{e} que je gardais un bon cap. 

J'aimerais remercier \'{e}galement mes encadrants de th\`{e}se, Michael et Hugues, pour m'avoir accompagn\'{e} lors de cette th\`{e}se. J'ai parfois \'{e}t\'{e} dur en n\'{e}gociations. Michael m'a beaucoup apport\'{e} au point de vue communication, comment \'{e}crire un papier, comment articuler une pr\'{e}sentation, comment faire un beau tweet. Quant \`{a} Hugues, on ne s'est pas beaucoup vus mais il a pu apporter un regard neuf souvent utile. Je le remercie aussi de m'avoir mis en contact avec Bjarni avec qui j'ai pass\'{e} quelques mois \`{a} Aarhus au Danemark en tant que doctorant visiteur, et o\`{u} je vais continuer un postdoc pour les deux prochaines ann\'{e}es.

J'aimerais aussi remercier tous les membres de l'\'{e}quipe et du laboratoire pour leur bon humeur. Cela va de m\^{e}me pour l'\'{e}quipe de Hugues \`{a} l'Institut Pasteur, ainsi que les chercheurs \`{a} Aarhus au Danemark. Particuli\`{e}rement, Keurcien pour, entre autres, les soir\'{e}es "bi\`{e}re, saucisson, macdo, chartreuse". J'aimerais remercier Nicolas pour sa mise \`{a} disposition et sa gestion des serveurs de l'\'{e}quipe, que j'ai beaucoup utilis\'{e}s pendant la derni\`{e}re ann\'{e}e de ma th\`{e}se. J'aimerais aussi remercier Magali et Michael pour m'avoir aid\'{e} \`{a} d\'{e}marrer le groupe R de Grenoble. Cela fait d\'{e}j\`{a} maintenant deux ans qu'il tourne, merci \`{a} tous ceux qui ont pr\'{e}sent\'{e} et particip\'{e}, et merci \`{a} Matthieu d'avoir repris le flambeau quand je suis parti au Danemark.

Enfin, merci \`{a} ma ch\'{e}rie, Sylvie, de m'avoir support\'{e} tout au long de cette th\`{e}se. J'ai eu un gros coup de mou \`{a} la moiti\'{e}. Aussi, les \textquotedblleft{}bon j'en ai marre de travailler, je vais faire la sieste\textquotedblright{} \`{a} 15h alors qu'elle travaillait jusqu'\`{a} 18h30, ce n'\'{e}tait pas forc\'{e}ment tr\`{e}s sympa. Et merci d'avoir accept\'{e} de partir au Danemark avec moi. Un dernier mot pour mes parents, pas forc\'{e}ment pour la th\`{e}se, mais pour tout mon parcours scolaire. Ils ont toujours fait en sorte que je ne manque rien, ce qui m'a permis de faire mes \'{e}tudes dans les meilleures conditions possibles. Merci.




\clearpage

\tableofcontents


\clearpage

\chapter{Introduction}


\includepdfset{offset=-.25in 0in,noautoscale,scale=0.83,pages={-},pagecommand={}}
\input{intro.tex}


\chapter[R packages for analyzing genome-wide data]{Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr}

\input{chap1.tex}


\chapter[Efficient penalized regression for PRS]{Efficient Implementation of Penalized Regression for Genetic Risk Prediction}

\input{chap2.tex}


\chapter[Making the most of Clumping and Thresholding]{Making the most of Clumping and Thresholding for polygenic scores}

\input{chap3.tex}


\chapter{Conclusion and Discussion}

\input{conclu.tex}


\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{natbib}
\bibliography{refs}


\appendix

\chapter{Code optimization based on linear algebra}

\input{appendix.tex}

\afterpage{\blankpage}


\end{document}
