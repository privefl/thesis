\section{Summary of the article}

\subsection{Introduction}

Most of the time, only summary statistics for a GWAS dataset are available, i.e.\ the estimated effect sizes and p-values for each variant of the dataset. Because of the availability of such data en masse, specific methods using those summary data have been developed for a wide range of applications \cite[]{pasaniuc2014fast,vilhjalmsson2015modeling,bulik2015ld,pasaniuc2017dissecting,speed2018sumher}. Moreover, methods using summary statistics data are usually fast and easy to use, making them even more appealing to researchers.
One of these summary statistics based methods applicable for polygenic prediction is Clumping and Thresholding (C+T).
When on limited sample size of individual-level data are available (as opposed to summary statistics), C+T provides a competitive method for deriving predictive polygenic risk scores \cite[]{prive2019efficient}.

C+T is the simplest and most widely-used method for constructing PRS based on summary statistics and has been used for many years now. The idea behind C+T is simple because it directly uses weights learned from GWAS; it further removes SNPs as one does when reporting hits from GWAS, i.e.\ only SNPs that pass the genome-wide threshold (p-value thresholding) and that are independent association findings (clumping) are reported.
In GWAS, it is commonly accepted to use a p-value threshold of $5 \times 10^{-8}$ when reporting significant findings, yet for prediction purposes, including less significant SNPs can substantially improve predictive performance \cite[]{purcell2009common}.

Therefore, when using C+T, one has to choose a p-value threshold that balances between removing informative variants when using a stringent p-value threshold and adding too much noise in the score by including too many variants with no effect. The clumping step aims
at removing redundancy in included effects that is simply due to linkage disequilibrium (LD) between variants. Yet, clumping may as well remove independently predictive variants in nearby regions; to balance this, C+T uses as hyper-parameter a threshold on correlation between variants included. 
Thus, C+T users must choose hyper-parameters of C+T well if they want to maximize predictive performance of the polygenic score derived.
Usually, users use some default value for these parameters, expect for the p-value threshold, for which they look at different values and choose the one maximizing predictive ability in some training set.

\subsection{Methods}

We implement an efficient way to compute many C+T scores corresponding to many different sets of hyper-parameters for C+T. This is now part of R package bigsnpr \cite[]{prive2018efficient}. 
The 4 parameters we vary are the correlation threshold of clumping, the window size for looking at correlation, the p-value threshold and the imputation accuracy threshold when using imputed variants.
In total, we investigate 5600 different sets of hyper-parameters for C+T.

We also derive a C+T score for each chromosome separately, resulting in 123,200 different scores.
We propose to use stacking, i.e.\ we fit a penalized regression of these scores and learn an optimal linear combination of those scores instead of only choosing the best one \cite[]{breiman1996stacked}.
We hypothesize that Stacked Clumping and Thresholding (SCT) has the potential to make C+T more flexible and to increase its predictive performance.
Moreover, SCT results in a linear model from which we can derive an unique vector of coefficients to be used for testing in unseen individuals.

\subsection{Results}

We test 6 different simulation scenarios using the UK Biobank dataset. We also derive PRS for 8 common diseases using external summary statistics from published GWAS and dividing the UK Biobank data into training and test sets.
Investigating more hyper-parameters for C+T (we call this maxCT) instead of using standard values for these hyper-parameters (we call this stdCT) consistently improves predictive performance in simulations and real data applications.
This makes C+T competitive to state-of-the-art methods like lassosum \cite[]{mak2017polygenic}.
Moreover, SCT often provides substantial predictive performance improvement over maxCT by using different weights from those reported from the GWAS.

\subsection{Discussion}

We provide an efficient way to compute C+T scores for many different hyper-parameters values in R package bigsnpr. We show that fine-tuning hyper-parameters of C+T improves its predictive performance as compared to using some default values for clumping. Investigating 8 different disease phenotypes, we show that the optimal C+T hyper-parameters for those traits are very different, probably because these diseases have different architectures.

Instead of choosing one set of hyper-parameters that maximizes predictive performance in a training set, we propose instead to learn a combination of many C+T scores, corresponding to different sets of hyper-parameters.
This extension of C+T that we call SCT (Stacked C+T) makes C+T more flexible.
Moreover, we implement the possibility for an user of SCT to define their own groups of variants. This opens many possibilities for SCT. For example, we could derive and stack C+T scores for two related but different GWAS summary statistics, we could use external information such as functional annotations, or we could learn to differentiate between two genetically different phenotypes with similar symptoms such as type 1 and type 2 diabetes.


\section{Article and supplementary materials}

The following article is available as a preprint in \textit{bioRxiv}	\footnote{\url{https://doi.org/}}.
